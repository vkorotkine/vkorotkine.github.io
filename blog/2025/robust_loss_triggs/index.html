<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> outlier rejection in nonlinear least squares | Vassili Korotkine </title> <meta name="author" content="Vassili Korotkine"> <meta name="description" content="an aesthetic derivation"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://vkorotkine.github.io/blog/2025/robust_loss_triggs/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "outlier rejection in nonlinear least squares",
            "description": "an aesthetic derivation",
            "published": "May 12, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Vassili</span> Korotkine </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">notes </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>outlier rejection in nonlinear least squares</h1> <p>an aesthetic derivation</p> </d-title> <d-article> <p>In robotics, before anything else, the robot must estimate its <strong>state</strong>. An aerial vehicle must know its position, velocity, and heading before planning its path and creating a control law to follow it. An autonomous vehicle must know its velocity and the position of surrounding vehicles. Even a Roomba vacuum cleaner must know its location relative to the household cat in order to avoid an early demise at the hands of General Mittens. The <strong>state</strong> is thus whatever quantity of interest must be estimated for subsequent safe, reliable robot operation.</p> <p>Furthermore, the state is never known perfectly but rather <strong>estimated</strong> from sensor measurements, which are themselves imperfect. An accelerometer is corrupted by noise; a camera can suffer from motion blur; feature points on images may be incorrectly associated across frames to yield outliers. The problem is thus statistical in nature, and is often solved in a Maximum A Posteriori optimization framework, which takes the form of a nonlinear least squares problem. For “well-behaved” problems, where there are no outliers, Gaussian sensor models are used. In cases where outliers are present, robust losses are used to address the effect of the outliers’ high residuals on the solution quality.</p> <h1 id="nonlinear-least-squares">Nonlinear Least Squares</h1> <p>For Gaussian sensor models, the Maximum A Posteriori problem takes the nonlinear least squares form</p> <p> \begin{align} \hat{\mathbf{x}}&amp;=\text{argmin}_{\mathbf{x}} J(\mathbf{x}) = \text{argmin}_{\mathbf{x}} \sum_{i=1}^N \frac{1}{2} \mathbf{e}_i^T(\mathbf{x}) \mathbf{e}_i(\mathbf{x}), \end{align} </p> <p>where each $\mathbf{e}_i(\mathbf{x})$ is a nonlinear, vector function of the state $\mathbf{x}$, and usually corresponds to the difference between <strong><em>expected</em></strong> (based on the sensor model) and <strong><em>actual</em></strong> value of the $i$’th received measurement.</p> <p>The Newton optimization update at an iterate $\mathbf{x}^k$ is derived by minimizing a quadratic expansion expanded around the current iterate to yield</p> <p> \begin{align} \frac{\partial J}{\partial \mathbf{x} \partial \mathbf{x}^T} \Delta \mathbf{x} = -\frac{\partial J}{\partial \mathbf{x}^T}. \end{align} </p> <p>For the nonlinear-least-squares form of the loss $J(\mathbf{x})$, the Jacobian and Hessian become</p> <p> \begin{align} \frac{\partial J}{\partial \mathbf{x}^T} &amp;= \sum_{i=1}^N \frac{\partial J}{\partial \mathbf{e}_i} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}} = \sum_{i=1}^N \mathbf{e}_i^T \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}}, \\ \frac{\partial J}{\partial \mathbf{x} \partial \mathbf{x}^T} &amp;= \sum_{i=1}^N \left(\frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}} +\sum_{j=1}^{n_{e_i}} e_{i,j} \frac{\partial ^2 e_{i,j}}{\partial \mathbf{x} \partial \mathbf{x}^T} \right). \end{align} </p> <p>The second term in the Hessian sum is neglected to yield the Gauss-Newton update,</p> <p> \begin{align} \left(\sum_{i=1}^N \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}}\right) \Delta \mathbf{x} &amp;= \sum_{i=1}^N \mathbf{e}_i^T \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}}. \end{align} </p> <p>Typically we see all the error terms stacked into one big error, which yields a cleaner expression. However, for the sake of the robust loss derivation later on, we will keep them separated.</p> <p>As an aside, the convenient way to derive/check vector derivative identities is through expanding the matrix operation as a sum, carrying out the differentiation, and reassembling the output as a matrix expression. For example, the Jacobian of the quadratic form $\mathbf{e}^T \mathbf{e}$ may be derived by writing the definition of the Jacobian, and keeping in mind that we are working with a scalar such that it only has one row,</p> <p> \begin{align} \left . \frac{\partial \mathbf{e}^T \mathbf{e}}{\partial \mathbf{e}} \right|_{1j} &amp;= \frac{\partial}{\partial e_j} \sum_{i=1}^N e_i^2 = 2e_j, \end{align} </p> <p>which may be reassembled back into matrix form as</p> <p> \begin{align} \frac{\partial \mathbf{e}^T \mathbf{e}}{\partial \mathbf{e}} &amp;= 2\mathbf{e}^T. \end{align} </p> <h1 id="robust-loss">Robust Loss</h1> <p>In the case of robust losses, a robust loss $\rho_i: \mathbb{R}\rightarrow \mathbb{R}$ is assigned to each quadratic form $f_i=\frac{1}{2}\mathbf{e}_i^T\mathbf{e}_i$, which downweighs the effects of very high residuals. See <d-cite key="Barron17"></d-cite> for examples of robust loss functions. The Maximum A Posteriori problem is</p> <p> \begin{align} \hat{\mathbf{x}}&amp;=\text{argmin}_{\mathbf{x}} J(\mathbf{x}) = \text{argmin}_{\mathbf{x}} \sum_{i=1}^N \rho_i\left(\frac{1}{2}\mathbf{e}_i^T(\mathbf{x}) \mathbf{e}_i(\mathbf{x})\right) \end{align} </p> <p>The Jacobian of the loss is given by</p> <p> \begin{align} \frac{\partial J}{\partial \mathbf{x}^T} &amp;= \sum_{i=1}^N \frac{\partial J}{\partial \mathbf{e}_i} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}} = \sum_{i=1}^N\frac{\partial \rho_i}{\partial f_i}\mathbf{e}_i^T \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}}, \end{align} </p> <p>while the Hessian of the loss is given by</p> <p> \begin{align} \frac{\partial J}{\partial \mathbf{x} \partial \mathbf{x}^T} &amp;= \left( \frac{\partial}{\partial \mathbf{x}} \frac{\partial J}{\partial \mathbf{x}^T}\right)^T \\ &amp;= \left( \frac{\partial}{\partial \mathbf{x}} \sum_{i=1}^N \left( \frac{\partial \rho_i}{\partial f_i} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \mathbf{e}_i \right) \right)^T. \end{align} </p> <p>The derivative of the summand is obtained by a double application of the product rule, such that</p> <p> \begin{align} \frac{\partial}{\partial \mathbf{x}} \left( \frac{\partial \rho_i}{\partial f_i} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \mathbf{e}_i \right) &amp;= \frac{\partial}{\partial \mathbf{x}} \left(\frac{\partial \rho_i}{\partial f_i}\right) \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \mathbf{e}_i + \frac{\partial \rho_i}{\partial f_i} \left( \left( \frac{\partial}{\partial \mathbf{x}} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \right) \mathbf{e}_i + \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}} \right). \end{align} </p> <p>The key step in the Gauss-Newton iteration, which is carried over to the robust loss case, is in neglecting the second order derivatives of $\mathbf{e}_i$, such that $\frac{\partial}{\partial \mathbf{x}} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T}\approx \mathbf{0}$. Furthermore, the chain rule must be applied since since $\frac{\partial \rho_i}{\partial f_i}$ is itself a function of $\mathbf{x}$. This yields</p> <p> \begin{align} \frac{\partial}{\partial \mathbf{x}} \left( \frac{\partial \rho_i}{\partial f_i} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \mathbf{e}_i \right) &amp;= \frac{\partial}{\partial \mathbf{x}} \left(\frac{\partial \rho_i}{\partial f_i}\right) \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \mathbf{e}_i + \frac{\partial \rho_i}{\partial f_i} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}} \\ &amp;= \frac{\partial^2 \rho_i}{\partial f_i^2}\mathbf{e}_i^T \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \mathbf{e}_i + \frac{\partial \rho_i}{\partial f_i} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}}. \end{align} </p> <p>The overall Hessian is obtained by substituting the summand expression we just derived back into the sum,</p> <p> \begin{align} \frac{\partial J}{\partial \mathbf{x} \partial \mathbf{x}^T} &amp;= \left( \frac{\partial}{\partial \mathbf{x}} \frac{\partial J}{\partial \mathbf{x}^T}\right)^T \\ &amp;= \sum_{i=1}^N \frac{\partial^2 \rho_i}{\partial f_i^2}\mathbf{e}_i^T \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \mathbf{e}_i + \frac{\partial \rho_i}{\partial f_i} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}}. \end{align} </p> <p>The first term in the sum is called the <strong>Triggs correction</strong> <d-cite key="triggs1999bundle"></d-cite>. Neglecting it yields the iteratively reweighted least squares approach, where the Newton step is given by</p> <p> \begin{align} \sum_{i=1}^N \frac{\partial \rho_i}{\partial f_i} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}^T} \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}} \Delta \mathbf{x} &amp;= \sum_{i=1}^N\frac{\partial \rho_i}{\partial f_i}\mathbf{e}_i^T \frac{\partial \mathbf{e}_i}{\partial \mathbf{x}} \end{align} </p> <p>Comparing this expression to the Gauss-Newton case clarifies why it is called iteratively reweighted least squares. For each factor with a robust loss, we just consider the <em>non-weighted</em> error, and then just weigh it by $\sqrt{\frac{\partial \rho_i}{\partial f_i}}$, which yields exactly the nonlinear-least-squares update for the robust loss case we just derived. This is done for every factor separately. We can notice that if only one factor is used, the $\sqrt{\frac{\partial \rho_i}{\partial f_i}}$’s cancel out.</p> <h1 id="remarks">Remarks</h1> <p>Different robust losses may be used, and correspond to specific heavy-tailed distributions if we work “backward” to the MAP problem from the negative log-likelihood. A Cauchy loss corresponds to a Cauchy distribution assumed on sensor noise. However, the motivation for using robust losses is usually not any kind of rigorous statistical argument. We do not consider the statistical properties of erroneous feature associations across camera frames to determine the “right” robust loss. Rather, we use trial and error. It works because it works, and because we have nice methods for solving the resulting problem. This somewhat echoes a theme I’ve seen on Ben Recht’s substack. We often work with given models not because they are the most accurate or rigorous ones. Rather, we work with them because we are able to solve them. Afterward, we can try to justify why these models correspond to reality.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-05-02-robust.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'vkorotkine/vkorotkine.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Vassili Korotkine. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?6fb1b44ca0fe71fcbff81b772136a785"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>