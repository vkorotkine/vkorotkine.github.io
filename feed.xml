<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://vkorotkine.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://vkorotkine.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-02T13:52:31+00:00</updated><id>https://vkorotkine.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">nonlinear noise in nonlinear-least-squares optimization</title><link href="https://vkorotkine.github.io/blog/2025/batch-noise-linearization/" rel="alternate" type="text/html" title="nonlinear noise in nonlinear-least-squares optimization"/><published>2025-03-19T00:00:00+00:00</published><updated>2025-03-19T00:00:00+00:00</updated><id>https://vkorotkine.github.io/blog/2025/batch-noise-linearization</id><content type="html" xml:base="https://vkorotkine.github.io/blog/2025/batch-noise-linearization/"><![CDATA[<p>Batch optimization methods are ubiquitous in robotics. We would like to solve for some robot states given some measurements. To do this, we create an error that quantifies the difference between <strong><em>the measurement we expect</em></strong> (that depends on the state) and the <strong><em>the measurement we receive</em></strong> (a given value from a sensor, and does not depend on the robot state).</p> <p>Consider a single received measurement $\mathbf{y}$, the sensor model for which is $\mathbf{y}=\mathbf{g}(\mathbf{x})+\mathbf{v}$, where $\mathbf{v}\sim \mathcal{N}(\mathbf{0}, \mathbf{R})$ is Gaussian distributed noise with covariance $\mathbf{R}$. The robot state estimate is obtained by solving the Max A Posteriori problem \begin{equation} \hat{\mathbf{x}}=\text{argmax}_{\mathbf{x}} p(\mathbf{x}|\mathbf{y}), \end{equation}</p> <p>Since there are no priors, and discarding the normalization constant, Bayes’ rule leaves us with \begin{equation} \hat{\mathbf{x}}=\text{argmax}_{\mathbf{x}} p(\mathbf{y}|\mathbf{x}). \end{equation}</p> <p>which is equivalent to minimizing the negative log-likelihood as \begin{equation} \hat{\mathbf{x}}=\text{argmin}_{\mathbf{x}} -\log p(\mathbf{y}|\mathbf{x}). \end{equation}</p> <p>The form for $p(\mathbf{y}|\mathbf{x})$ is Gaussian, meaning that \begin{equation} p(\mathbf{y}|\mathbf{x}) = \alpha \exp(-(\mathbf{y}-\mathbf{g}(\mathbf{x}))^{\text{trans}}\mathbf{R}^{-1}(\mathbf{y}-\mathbf{g}(\mathbf{x}))), \end{equation}</p> <p>where $\alpha$ is a normalization constant. By defining $\mathbf{e}(\mathbf{x})=\mathbf{y}-\mathbf{g}(\mathbf{x})$, the negative log-likelihood minimization can be written \begin{equation} \hat{\mathbf{x}}=\text{argmin}_{\mathbf{x}} \mathbf{e}(\mathbf{x}) \mathbf{R}^{-1} \mathbf{e}(\mathbf{x}). \end{equation}</p> <p>This is pretty much in the nonlinear-least-squares form that is used by solvers such as Ceres. We linearize the error $\mathbf{e}(\mathbf{x})$ to construct successive Taylor series approximations to our loss and (hopefully) reach the minimum we want.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/linearization_everywhere-480.webp 480w,/assets/img/blogs/linearization_everywhere-800.webp 800w,/assets/img/blogs/linearization_everywhere-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blogs/linearization_everywhere.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A favorite engineering pastime. </div> <p>However, what happens when the noise enters nonlinearly into the measurement model? Formally, \begin{equation} \mathbf{y}=\mathbf{g}(\mathbf{x}, \mathbf{v}) \quad \mathbf{v}\sim \mathcal{N}(\mathbf{0}, \mathbf{R}). \end{equation}</p> <p>The measurement likelihood $p(\mathbf{y}|\mathbf{x})$ is non-Gaussian and forming the error \begin{equation} \mathbf{e}(\mathbf{x})=\mathbf{y}-\mathbf{g}(\mathbf{x}, \mathbf{v}), \end{equation}</p> <p>gets us nowhere. So we linearize and write \begin{equation} \mathbf{y}=\mathbf{g}(\mathbf{x}) + \mathbf{v}\quad \mathbf{v}\sim \mathcal{N}(\mathbf{0}, \mathbf{L} \mathbf{R} \mathbf{L}^{\text{T}}), \end{equation}</p> <p>where $\mathbf{L}$ is the Jacobian of the measurement model with respect to the noise variable evaluated at the current state estimate $\bar{\mathbf{x}}$, \begin{equation} \mathbf{L} = \left. \frac{\partial \mathbf{g}}{\partial \mathbf{x}}\right|_{\bar{\mathbf{x}}}. \end{equation}</p> <p>This is different from the linearization approximation that we use in methods like Gauss-Newton, where we first pose the problem then use iterative methods to solve it. <em>_before even going into the optimizer_</em> we make a linearization approximation to the loss. Whether this causes issues depends on the application. We can get funny situations where $\mathbf{L}$ is not full rank, causing a singular covariance. </p>]]></content><author><name></name></author><summary type="html"><![CDATA[linearization everywhere]]></summary></entry><entry><title type="html">discretization of continuous-time noise and process models</title><link href="https://vkorotkine.github.io/blog/2025/discrete-time-noise/" rel="alternate" type="text/html" title="discretization of continuous-time noise and process models"/><published>2025-03-19T00:00:00+00:00</published><updated>2025-03-19T00:00:00+00:00</updated><id>https://vkorotkine.github.io/blog/2025/discrete-time-noise</id><content type="html" xml:base="https://vkorotkine.github.io/blog/2025/discrete-time-noise/"><![CDATA[<p>Physical systems are continuous. However, we work with discrete-time models that are amenable to digital computers. Therefore, we create continuous models of the physical systems that we then discretize.</p> <p>This becomes particularly interesting in the case of sensor noise, since there is a randomness that needs to be modeled. Typically we use white noise models. However, while the discrete-time white noise model is fairly straightforward (a Gaussian distribution for every noise sample), the continuous-time model is really not, and deriving the transition from continuous to discrete time for white noise is nuanced.</p> <p>A key aspect in this dilemma is that it’s hard to decouple the dynamics of the system we are considering from the random process noise properties. For instance, a gyroscope may have some continuous noise characteristics of its own. Yet, when we consider discrete-time properties, we need to look at the effect of time, and thus the overall system dynamics. This is only really tractable for linear systems (and thus, linearizations).</p> <p>The post is split into three parts. The first part on random processes essentially covers terminology. We go over why the term power spectral density is used interchangeably with continuous-time covariance. The second part covers the “Linearize then Discretize” method, which obtains discretizations by lumping in the effects of system dynamics with effect of random process noise. The first and second parts are essentially summaries of relevant parts of Sec. 4.3.2, 4.4, 4.7 of <d-cite key="farrell2008navigation"></d-cite>. The third part is where things get really interesting. We consider continuous to discrete time noise “in a vacuum” and try to decouple the effect of system dynamics from the random process noise. There is a commonly used formula, which yields good results in practice, but as far there is no completely satisfying way to derive it from first principles. We go through some approaches that have been proposed and discuss them. </p> <h1 id="random-processes">Random Processes</h1> <p>The world operates in continuous time, yet our sensors provide discrete data. Furthermore, our sensors are noisy, which means we need to be able to reason about uncertainty and randomness in order to intelligently design algorithms for state estimation.</p> <p>A random process $\mathbf{v}(t)$ defines a probability density at every time $t$.</p> <p>The autocovariance of a random process $\mathbf{v}(t)$ is given by \begin{equation} \text{cov}(\mathbf{v}(t_1), \mathbf{v}(t_2))=\mathbb{E}[\mathbf{v}(t_1)\mathbf{v}(t_2)^\text{T}] - \mathbb{E}[\mathbf{v}(t_1)]\mathbb{E}[\mathbf{v}(t_2)^\text{T}]. \end{equation}</p> <p>Essentially we take a bunch of samples at different times $t_1, t_2$, subtract the means at those times, and see how the results vary together.</p> <p>In the context of sensor noise, a commonly used assumption is that noise is a <em>wide sense stationary</em> random process, where the <em>mean</em> and <em>variance</em> of the process are independent of time.</p> <p>For a <em>wide sense stationary</em> process, the autocovariance $\text{cov}(\mathbf{v}(t_1), \mathbf{v}(t_2))$ <em>only</em> depends on the time difference $\tau t=t_2-t_1$, \begin{equation} \text{cov}(\mathbf{v}(t_1), \mathbf{v}(t_2))=\mathbf{R}(t_2-t_1) = \mathbf{\tau}. \end{equation}</p> <p>The Power Spectral Density (PSD) of the process is given by \begin{equation} \mathbf{S}(j\omega)=\int_{-\infty}^\infty \mathbf{R}(\tau) \exp (-j\omega \tau) \text{d} \tau. \end{equation}</p> <p>The PSD describes the strength of the random process as different frequencies. For the scalar case, <em>white</em> noise is noise where the PSD is a constant for all frequencies, $\mathbf{S}(j\omega)=\mathbf{S}_w$. Technically, this kind of noise would have infinite power - but this works in practice.</p> <p>Using the inverse Fourier transform, white noise implies that the autocovariance is given by \begin{equation} \mathbf{R}_{\text{white}}(\tau)= \mathbf{S}_w \delta (\tau) = \mathbf{R}_c \delta (\tau). \end{equation} where $\delta(\tau)$ is the Dirac delta. For a long while I was confused by the fact that in sensor datasheets, the PSD is typically stated, and we read it off and use it as the continuous-time covariance for sensor noise. At first glance it does not make sense, since PSD is a frequency domain concept. However, with the assumption of white noise and a few lines of Fourier transforms, the PSD is shown to have the same value as the continuous-time noise covariance, denoted $\mathbf{R}_c$.</p> <h1 id="from-continuous-time-covariances-to-discrete-time">From Continuous-Time Covariances to Discrete Time</h1> <h2 id="linearize-then-discretize">Linearize Then Discretize</h2> <p>Consider a continuous-time system of the form \begin{equation} \dot{\mathbf{x}}=\mathbf{A}_c(t)\mathbf{x}(t)+\mathbf{L}_c(t)\mathbf{w}(t), \quad \mathbf{w}(t) \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}_c \delta(\tau)) \end{equation}</p> <p>with $\mathbf{w}(t)$ being white noise, a Gaussian random process with continuous-time covariance $\mathbf{Q}_c$. This system may for instance come from linearization of a nonlinear system. The goal is to find an equivalent discrete-time system of the form</p> <p>\begin{equation} \mathbf{x}_{k+1}=\mathbf{A}_k \mathbf{x}_k + \mathbf{w}_k, \quad \mathbf{w}_k \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}_d), \end{equation}</p> <p>which is more tractable to reason about on digital computers. The solution for $\mathbf{A}_k$ is fairly straightforward,</p> <p>\begin{equation} \mathbf{A}_k = \exp(\mathbf{A}_c \Delta t), \end{equation}</p> <p>where $\Delta t$ is the sampling period, and the matrix exponential is used. On the other hand, determining $\mathbf{Q}_d$ requires integration of the continuous-time dynamics to compute the discrete noise $\mathbf{w}_k$,</p> <p>\begin{equation} \mathbf{w}_k = \int_{-\infty}^\infty \exp (\mathbf{A} (t_{k+1}-\tau))\mathbf{L}_c (\tau) \mathbf{w}(\tau)\text{d}\tau \end{equation} The continuous-time covariance $\mathbf{Q}_d$ is then given by \begin{align} \mathbf{Q}_d&amp;=\mathbb{E}[\mathbf{w}_k \mathbf{w}_k^{\text{T}}], \end{align} which, after simplification, becomes </p> <p>\begin{equation} \mathbf{Q}_d = \int_{t_k}^{t_{k+1}} \exp(\mathbf{A_c}(t_{k+1}-\tau)) \mathbf{L}_c \mathbf{Q}_c \mathbf{L}_c^\text{T} \exp(\mathbf{A_c}(t_{k+1}-\tau))^{\text{T}} \text{d} \tau. \end{equation} </p> <p> The resulting $\mathbf{Q}_d$ is thus dependent on how the matrix exponential $\exp(\mathbf{A_c}(t_{k+1}-\tau))$ is computed. In some cases, it can be computed in closed form, giving an exact solution for $\mathbf{Q}_d$. It can also be approximated numerically using a Taylor series. The Taylor series approximation for $\exp(\mathbf{A}\Delta t)$ is given by </p> <p>\begin{equation} \exp(\mathbf{A}\Delta t)=\exp(\mathbf{A}_c\Delta t)=\mathbf{1}+\mathbf{A}_c\Delta t +\frac{1}{2}(\mathbf{A}_c\Delta t)^2+\frac{1}{3!}(\mathbf{A}_c\Delta t)^3 + \dots \end{equation} </p> <p>Using a zero’th order approximation, $\exp(\mathbf{A}_c\Delta t)\approx \mathbf{1}$, yields $\mathbf{Q}_d \approx \Delta t \mathbf{L}_c \mathbf{Q}_c \mathbf{L}_c^{\text{T}}$. Using the first four terms, a 3rd order Taylor series approximation, yields</p> <p>\begin{align} \mathbf{Q}_d&amp;\approx \mathbf{Q}_c\Delta t + (\mathbf{A}_c\mathbf{Q}_c + \mathbf{Q}_c \mathbf{A}_c^\text{T}) \frac{\Delta T^2}{2} +(\mathbf{A}_c^2\mathbf{Q}_c +2\mathbf{A}_c\mathbf{Q}_c\mathbf{A}^\text{T}+\mathbf{Q}(\mathbf{A}_c^\text{T})^2)\frac{\Delta T^3}{6}+\\ &amp;\quad(\mathbf{A}_c^3\mathbf{Q}_c+3\mathbf{A}_c^2\mathbf{Q}_c\mathbf{A}_c^\text{T} +3\mathbf{A}_c\mathbf{Q}_c(\mathbf{A}_c^\text{T})^2+\mathbf{Q}_c(\mathbf{A}_c^\text{T})^3)\frac{T^4}{24}. \end{align} </p> <h2 id="continuous-to-discrete-time-noise-in-a-vacuum">Continuous to Discrete Time Noise “In A Vacuum”</h2> <p>The above discussion is valid for a linear time varying system. This makes sense, for instance, when we linearize a nonlinear system and want to discretize the result. However, in some situations, we <em>already</em> have an exact discretization of the nonlinear system. In this situation, we <em>just</em> want the discrete-time analog of the continuous-time noise on the sensor, without looking at system dynamics - since we already have an exact discretization. For instance, consider the case of angular velocity kinematics with rotations,</p> <p>\begin{align} \dot{\mathbf{C}} &amp;= \mathbf{C} \boldsymbol{\omega}^\times \\ \mathbf{C}_{k+1} &amp;= \mathbf{C} \text{exp}({\Delta t\boldsymbol{\omega}_k}^\times), \end{align}</p> <p>where $\mathbf{C}$ is a cosine matrix describing the orientation of our robot, $\omega$ is an angular velocity measurement, and $\times$ is the cross product operator mapping to the Lie algebra of the space of rotations the $SO(3)$ group.</p> <p>The details are irrelevant and involve Lie groups, which is a big topic in and of itself. I have some <a href="/assets/pdf/notes/lie_group_doc.pdf">notes</a> on them and they are also covered in <d-cite key="barfoot2024state"></d-cite> and <d-cite key="sola2021microlietheorystate"></d-cite>. The point is that we have a real-world example where we have an exact discretization of the system already, without any assumptions on Taylor series truncations.</p> <p>The key aspect of this is that, in the section above, there are <em>two</em> discretizations happening. We are discretizing the linear time varying system <em>as well as</em> the continuous random process. However, in the current situation, we are <em>only</em> interested in the continuous random process. If we take the gyroscope noise with its given continuous-time noise characteristics, and we sample it at a given frequency: What will be the covariance on the resulting discrete-time sampled noise? This tends to be quite confusing, as in process noise derivations such as Sec. 4.7 of <d-cite key="farrell2008navigation"></d-cite> and Sec. 8.1.1 of <d-cite key="simon2006optimal"></d-cite> the linear system and random noise process discretizations are lumped in together. Sec. 8.1.2 of <d-cite key="simon2006optimal"></d-cite> is actually the relevant one for this.</p> <p>Formally, given Gaussian white noise \begin{equation} \mathbf{v}(t) \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}_c \delta (t_1-t_2)), \end{equation}</p> <p>what is the “equivalent” discrete-time white noise \begin{equation} \mathbf{v}_k \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}_d)? \end{equation}</p> <p>The “equivalent” is in quotes for good reason. For a Gaussian random process, <em>by definition</em>, \begin{equation} \mathbb{E}[\mathbf{v}(t_k) \mathbf{v}(t_k)^{\text{T}}] = \mathbb{E}[\mathbf{v}_k \mathbf{v}_k^{\text{T}}] = \mathbb{E}[\mathbf{Q}_c \delta (t_1-t_2)] = \mathbb{E}[\mathbf{Q}_c \delta (t_k-t_k)] = \mathbf{Q}_c. \end{equation}</p> <p>So, what is $\mathbf{v}_k$? It does not refer to the actual random process variable $\mathbf{v}(t_k)$. Rather, it refers to a separate $\mathbf{v}_k$ that behaves in a way that makes sense for us in an estimator.<br/> The widely used (and seemingly correct) answer is \begin{equation} \mathbf{Q}_d = \frac{\mathbf{Q}_c}{\Delta t}. \end{equation}</p> <p>However, tracking down exactly where it comes from and how it is derived is nontrivial. This is the equation given by <a href="https://github.com/ethz-asl/kalibr/wiki/IMU-Noise-Model">the IMU noise model section of Kalibr wiki</a>, which itself cites the appendix of J. Crassidis’ sigma point Kalman filtering paper <d-cite key="crassidis2006sigma"></d-cite>. The other source for this seem to be the 3D attitude estimation paper by N. Trawny and S. Roulemiotios <d-cite key="trawny2005indirect"></d-cite>, and Dan Simon’s optimal state estimation book <d-cite key="simon2006optimal"></d-cite>, which is itself the cited source in <d-cite key="trawny2005indirect"></d-cite>.</p> <h3 id="forward-euler-discretization-of-linear-system-and-direct-comparison">Forward Euler Discretization of Linear System And Direct Comparison</h3> <p>This derivation I have seen floating around, but I do not have a direct source for it. It’s the one that makes the most sense though.</p> <p>We take a linear system, discretize it using a forward Euler method, and then compare to the zero’th order approximation from the Linearize Then Discretize approach. We are thus able to separate the discretization of the system matrices from discretization of the random process.</p> <p>Formally, start with the, for now deterministic, linear system as follows. This corresponds to the first step of just discretizing the system matrices, without considering the random process aspect. Thus, for now, $\mathbf{v}$ is deterministic.</p> <p> \begin{align} \dot{\mathbf{x}}&amp;=\mathbf{A}_c \mathbf{x}+\mathbf{L}_c \mathbf{v}, \end{align} </p> <p>a forward Euler scheme with $\dot{\mathbf{x}}\approx \frac{\mathbf{x}_{k+1}-\mathbf{x}_k}{\Delta t}$ yields</p> <p> \begin{align} \mathbf{x}_{k+1}&amp;=\underbrace{(\mathbf{1}+\Delta t\mathbf{A}_c)}_{\mathbf{A}_d} \mathbf{x}_k+ \underbrace{\Delta t \mathbf{L}_c}_{\mathbf{L}_d} \mathbf{v}. \end{align} </p> <p>Now let us remember the fact that $\mathbf{v}$ is actually issued from a random process and consider the system</p> <p> \begin{align} \dot{\mathbf{x}}&amp;=\mathbf{A}_c \mathbf{x}+\mathbf{L}_c \mathbf{v}, \quad \mathbf{v} \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}_c \delta (t_1-t_2)), \end{align} </p> <p>such that the equivalent discrete-time system is given by</p> <p> \begin{align} \mathbf{x}_{k+1}&amp;=\underbrace{(\mathbf{1}+\Delta t\mathbf{A}_c)}_{\mathbf{A}_d} \mathbf{x}_k+ \underbrace{\Delta t \mathbf{L}_c}_{\mathbf{L}_d} \mathbf{v}_k, \quad \mathbf{v}_k\sim \mathcal{N}(\mathbf{0}, \mathbf{R}_d), \end{align} </p> <p>where now $\mathbf{v}_k$ is its own Gaussian random variable, with yet to be determined $\mathbf{R}_d$. This is equivalent to</p> <p> \begin{align} \mathbf{x}_{k+1}&amp;=\mathbf{A}_d \mathbf{x}_k+\mathbf{w}_k, \quad \mathbf{w}_k\sim \mathcal{N}(\mathbf{0}, \mathbf{L}_d\mathbf{R}_d\mathbf{L}_d^T), \end{align} </p> <p>$\mathbf{R}_d$ is used for the covariance as $\mathbf{Q}_d$ has been reserved for the $\mathbf{w}_k$ from the Linearize then Discretize section. However, $\mathbf{w}_k$ now corresponds to the $\mathbf{w}_k$ from the Linearize then Discretize section. Using the zero’th order approximation for $\mathbf{Q}_d$ and comparing to the $\mathbf{L}_d\mathbf{R}_d\mathbf{L}_d^T$ expression we obtained here gives</p> <p> \begin{align} \mathbf{L}_d\mathbf{R}_d\mathbf{L}_d^T &amp;= \Delta t \mathbf{L}_c \mathbf{Q}_c \mathbf{L}_c^{\text{T}} \\\\ \Delta t^2 \mathbf{L}_c\mathbf{R}_d\mathbf{L}_c^T &amp;= \Delta t \mathbf{L}_c \mathbf{Q}_c \mathbf{L}_c^{\text{T}} \\\\ \mathbf{R}_d &amp;= \mathbf{Q}_c/\Delta t. \end{align} </p> <p>We have reverse engineered the covariance on just the sensor noise itself, $\mathbf{R}_d$, by considering a first-order forward Euler discretization to separate out the system matrix discretization from the random process discretization. We knew what the result of the overall discretization should be from the zeroth order approximation in the Linearize and Discretize section. Then we compared the results of the two approaches and matched the right matrices together.</p> <p>This is the approach that made the most sense to me. However, it is very much roundabout. It does not fully answer the question of “If I just have a nonlinear discrete system with continuous covariances specified for my sensor, what is the discrete time covariance?”. Rather we have to go through linearizations.</p> <h3 id="constant-covariance-estimate-for-a-static-system">Constant Covariance Estimate For A Static System</h3> <p>This is the argument of Sec. 8.1.2 of <d-cite key="simon2006optimal"></d-cite>. I have a few big issues with the chain of logic presented in there. If anyone has good answers to these, please let me know.</p> <p>The argument is essentially as follows. We are trying to isolate the effect of noise, so we take a discrete-time system whose state does not change, and whose measurement is directly equal to the state. This is like taking a gyroscope, putting it on a table (such that the true angular velocity does not change), and considering the resulting measurement. We then say that if we apply the Kalman filter to this system, the error covariance should not change, since there is nothing time-changing about the system. This implies a specific form for the discrete-time covariance, which ends up being the same as the continuous-to-discrete-time conversion we seek. Formally, we start with the scalar system</p> <p> \begin{align} x_k &amp;= x_{k-1} \\ y_k &amp;= x_k + v_k, \quad v_k \sim \mathcal{N}(0, R_d). \end{align} </p> <p>We then apply a Kalman filter to do a correction at a given timestep. There is no uncertainty in the process model, and the correction boils down to linking subsequent covariances as</p> <p> \begin{equation} P_{k+1}=\frac{P_k R_d}{P_k + R_d}. \end{equation} </p> <p>Right off the bat, my first problem with the argument. This only holds for the scalar case. But okay. This then implies that the covariance at timestep $k$ is given by</p> <p> \begin{equation} P_{k}=\frac{P_0 R_d}{kP_0+R_d}, \end{equation} </p> <p>and we want to manipulate this. This is where their argument goes completely off the rails, at least in my understanding. We take the following limit,</p> <p> \begin{equation} \lim_{k\rightarrow \infty}P_{k}=\frac{R}{k}=\frac{R_dT}{k}. \end{equation} </p> <p>And this I completely do not understand. <strong><em>Why</em></strong> ?! Why do we have to push the initial covariance to infinity? How does this make sense? If someone understands please let me know. The argument then proceeds by setting \begin{equation} R_d=\frac{R_c}{T} \end{equation} to make the above expression independent of $T$.</p> <p>To be blunt, I do not think the argument presented in this book for the discrete to continuous time conversion (at least, when purely considering the measurement in Sec. 8.1.2) makes sense. It is possible I missed something major. If I did, please let me know by sending me a mail. That being said, this has a commonality with the previous approach, in that we specify the continuous-time to discrete-time covariance conversion based on behaviour we want from the estimator, and not from a pure mathematical consideration of the random process.</p> <h3 id="conclusion-for-continuous-to-discrete-time-noise-in-a-vacuum">Conclusion For Continuous to Discrete Time Noise “In A Vacuum”</h3> <p>I suspect there is no good explanation based on sampling or integrating the white-noise continuous Gaussian process. One can perhaps do some kind of averaging as mentioned in <a href="https://math.stackexchange.com/questions/3851352/is-there-a-continuous-time-stochastic-process-that-when-sampled-yields-discret">this Stack Overflow post.</a> However, purely in terms of sensor noise for a robot navigation sensor model, it seems there is no mathematically satisfying way to decouple the system dynamics from the random process. The best we can do is the forward Euler discretization comparison method.</p> ]]></content><author><name></name></author><summary type="html"><![CDATA[some bees in my bonnet on continuous-time to discrete-time white noise conversion]]></summary></entry><entry><title type="html">useful resources</title><link href="https://vkorotkine.github.io/blog/2025/useful-resources/" rel="alternate" type="text/html" title="useful resources"/><published>2025-02-25T11:00:00+00:00</published><updated>2025-02-25T11:00:00+00:00</updated><id>https://vkorotkine.github.io/blog/2025/useful-resources</id><content type="html" xml:base="https://vkorotkine.github.io/blog/2025/useful-resources/"><![CDATA[<p>The following is a small list of resources that I recommend to people occasionally.</p> <ul> <li>The <a href="https://www.incontrolpodcast.com/"> InControl podcast</a> hosted by Alberto Padoan. Control theory, math, robotics.</li> <li><a href="https://substack.com/@henryfarrell"> Henry Farrell’s substack</a>. Mix of policy and technology.</li> <li><a href="https://www.argmin.net/"> argmin blog</a> by Ben Recht. Optimization and machine learning.</li> <li><a href="https://hankyang.seas.harvard.edu/Semidefinite/"> Semidefinite optimization and relaxation book</a> by Heng Yang. Covers certifiably optimal algorithms in robotics.</li> <li><a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-696.pdf"> An introduction to inertial navigation</a> by Oliver J. Woodman.</li> <li><a href="https://docs.google.com/document/d/e/2PACX-1vQTvxZkYPbOq3VYKCfAy8hKs4wjwLOF6z_7LT5vDkDSgVmcOto15-yzmOVOi8uAaGVkWoPCg2FNHD-v/pub"> How to Read a Research Paper </a> by Dmitry Berenson at University of Michigan. Written for the robotics field specifically.</li> <li><a href="https://rpg.ifi.uzh.ch/docs/IROS18_Zhang.pdf"> A Tutorial on Quantitative Trajectory Evaluation for Visual(-Inertial) Odometry </a> by Zichao Zhang and Davide Scaramuzza. Useful reference for error metrics in robot navigation.</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[The following is a small list of resources that I recommend to people occasionally. The InControl podcast hosted by Alberto Padoan. Control theory, math, robotics. Henry Farrell’s substack. Mix of policy and technology. argmin blog by Ben Recht. Optimization and machine learning. Semidefinite optimization and relaxation book by Heng Yang. Covers certifiably optimal algorithms in robotics. An introduction to inertial navigation by Oliver J. Woodman. How to Read a Research Paper by Dmitry Berenson at University of Michigan. Written for the robotics field specifically. A Tutorial on Quantitative Trajectory Evaluation for Visual(-Inertial) Odometry by Zichao Zhang and Davide Scaramuzza. Useful reference for error metrics in robot navigation.]]></summary></entry><entry><title type="html">random code tidbits</title><link href="https://vkorotkine.github.io/blog/2025/python-float-formatters/" rel="alternate" type="text/html" title="random code tidbits"/><published>2025-01-30T11:00:00+00:00</published><updated>2025-01-30T11:00:00+00:00</updated><id>https://vkorotkine.github.io/blog/2025/python-float-formatters</id><content type="html" xml:base="https://vkorotkine.github.io/blog/2025/python-float-formatters/"><![CDATA[<p>I often forget the details of some basic Python syntax. The goal of this post is to collect them somewhere. It is a pure convenience post as these are easily googleable. I expand and refine on these as time allows.</p> <h2 id="python-function-tidbits">Python Function Tidbits</h2> <p>Formatting a Python float with a given number of significant figures,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">number</span> <span class="o">=</span> <span class="mf">1.2345</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number formatted to 2 decimal places: </span><span class="si">{</span><span class="n">number</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>Writing a string to file,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_string</span> <span class="o">=</span> <span class="sh">"</span><span class="s">lie_groups</span><span class="sh">"</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">Output.txt</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">my_string</span><span class="p">)</span>
</code></pre></div></div> <p>Read/write with pickle,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_dict</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">euler.pickle</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">wb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">test_dict</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle</span><span class="p">.</span><span class="n">HIGHEST_PROTOCOL</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">euler.pickle</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">rb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</code></pre></div></div> <p>Number formats:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"</span><span class="s">.1e</span><span class="sh">"</span> <span class="c1">#: scientific notation with 1 decimal point (standard form)
</span><span class="sh">"</span><span class="s">.2f</span><span class="sh">"</span> <span class="c1">#: 2 decimal places
</span><span class="sh">"</span><span class="s">.3g</span><span class="sh">"</span> <span class="c1">#: 3 significant figures
</span><span class="sh">"</span><span class="s">.4%</span><span class="sh">"</span> <span class="c1">#: percentage with 4 decimal places
</span></code></pre></div></div> <p>with more details <a href="https://docs.python.org/3/library/string.html#formatspec">here</a>.</p> <h2 id="plotting">Plotting</h2> <p>Plotting timestamps as vertical lines,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span><span class="p">:</span> <span class="n">plt</span><span class="p">.</span><span class="n">Axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">vlines</span><span class="p">([</span><span class="n">stamps_rel_pos</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Measurement stamps</span><span class="sh">"</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="pandas-dataframe-manipulation">Pandas Dataframe Manipulation</h2> <p>Nicely formatting a multilevel pandas dataframe to input into a paper. Given a dataframe with columns Dims and Method, as well as some metrics of interest, we create a table that breaks down these metrics by Dims and Method.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>


<span class="k">def</span> <span class="nf">highlight_min</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">highlight_max</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    func : function
        ``func`` should take a Series if ``axis`` in [0,1] and return a list-like
        object of same length, or a Series, not necessarily of same length, with
        valid index labels considering ``subset``.
        ``func`` should take a DataFrame if ``axis`` is ``None`` and return either
        an ndarray with the same shape or a DataFrame, not necessarily of the same
        shape, with valid index and columns labels considering ``subset``.
    </span><span class="sh">"""</span>
    <span class="n">attr</span> <span class="o">=</span> <span class="sh">"</span><span class="s">textbf:--rwrap;</span><span class="sh">"</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">ndim</span>
    <span class="c1"># ndim = len(data.index.names)
</span>    <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Series from .apply(axis=0) or axis=1
</span>        <span class="k">if</span> <span class="n">highlight_max</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">:</span>
            <span class="n">is_min</span> <span class="o">=</span> <span class="n">data</span> <span class="o">==</span> <span class="n">data</span><span class="p">.</span><span class="nf">min</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">is_min</span> <span class="o">=</span> <span class="n">data</span> <span class="o">==</span> <span class="n">data</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span>
        <span class="n">ret_val</span> <span class="o">=</span> <span class="p">[</span><span class="n">attr</span> <span class="k">if</span> <span class="n">v</span> <span class="k">else</span> <span class="sh">""</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">is_min</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">highlight_max</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">:</span>
            <span class="n">is_min</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">transform</span><span class="p">(</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">)</span> <span class="o">==</span> <span class="n">data</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">is_min</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">transform</span><span class="p">(</span><span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="p">)</span> <span class="o">==</span> <span class="n">data</span>
        <span class="n">ret_val</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span>
            <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">is_min</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="sh">""</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="n">data</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">data</span><span class="p">.</span><span class="n">columns</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">ret_val</span>


<span class="k">def</span> <span class="nf">format_multiindexed_df</span><span class="p">(</span>
    <span class="n">df_metric</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span>
    <span class="n">floating_point_format_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="n">column_format</span><span class="o">=</span><span class="sh">"</span><span class="s">|*{7}{c|}</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">min_columns</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">max_columns</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">drop_columns</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Dims</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">NEES</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Method</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Run Name</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Case</span><span class="sh">"</span><span class="p">],</span>
<span class="p">):</span>

    <span class="n">df_metric</span> <span class="o">=</span> <span class="n">df_metric</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">drop_columns</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">df_metric</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">drop_columns</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">min_columns</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">df_styled</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">style</span> <span class="o">=</span> <span class="n">df_metric</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span>
            <span class="n">highlight_min</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="n">min_columns</span>
        <span class="p">)</span>
    <span class="n">df_styled</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">style</span> <span class="o">=</span> <span class="n">df_styled</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span>
        <span class="n">highlight_min</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="n">max_columns</span><span class="p">,</span> <span class="n">highlight_max</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span>

    <span class="n">df_styled</span> <span class="o">=</span> <span class="n">df_styled</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">floating_point_format_dict</span><span class="p">)</span>

    <span class="n">latex_string</span> <span class="o">=</span> <span class="n">df_styled</span><span class="p">.</span><span class="nf">to_latex</span><span class="p">(</span><span class="n">column_format</span><span class="o">=</span><span class="n">column_format</span><span class="p">,</span> <span class="n">hrules</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">latex_string</span> <span class="o">=</span> <span class="n">latex_string</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span>
        <span class="sh">"</span><span class="se">\\\n</span><span class="s">\end{tabular}</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="se">\\</span><span class="s"> \hline </span><span class="se">\n</span><span class="s">\end{tabular}</span><span class="sh">"</span>
    <span class="p">)</span>
    <span class="n">latex_string</span> <span class="o">=</span> <span class="n">latex_string</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span>
        <span class="sh">"</span><span class="se">\\\n</span><span class="s">\end{tabular}</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="se">\\</span><span class="s"> \hline </span><span class="se">\n</span><span class="s">\end{tabular}</span><span class="sh">"</span>
    <span class="p">)</span>

    <span class="c1"># latex_string = latex_string.replace("{r}", "{c|}")
</span>    <span class="n">latex_string</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">"</span><span class="se">\\</span><span class="s">\.*rule</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="se">\\</span><span class="s">\hline</span><span class="sh">"</span><span class="p">,</span> <span class="n">latex_string</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">latex_string</span><span class="p">)</span>

<span class="n">index</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">MultiIndex</span><span class="p">.</span><span class="nf">from_frame</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="sh">"</span><span class="s">Dims</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Method</span><span class="sh">"</span><span class="p">]])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">set_index</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">rename</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">Avg Iterations</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Avg Iter.</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="nf">format_multiindexed_df</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">RMSE (deg)</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">{:,.2f}</span><span class="sh">"</span><span class="p">.</span><span class="nb">format</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">RMSE (m)</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">{:,.2f}</span><span class="sh">"</span><span class="p">.</span><span class="nb">format</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">ANEES</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">{:,.2f}</span><span class="sh">"</span><span class="p">.</span><span class="nb">format</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Time (s)</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">{:,.2f}</span><span class="sh">"</span><span class="p">.</span><span class="nb">format</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Avg Iter.</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">{:,.2f}</span><span class="sh">"</span><span class="p">.</span><span class="nb">format</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">min_columns</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">RMSE (deg)</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">RMSE (m)</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">ANEES</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Avg Iter.</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Time (s)</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">max_columns</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="p">)</span>

</code></pre></div></div>]]></content><author><name></name></author><category term="code"/><summary type="html"><![CDATA[useful to have on hand]]></summary></entry><entry><title type="html">useful identities</title><link href="https://vkorotkine.github.io/blog/2021/identities/" rel="alternate" type="text/html" title="useful identities"/><published>2021-05-22T00:00:00+00:00</published><updated>2021-05-22T00:00:00+00:00</updated><id>https://vkorotkine.github.io/blog/2021/identities</id><content type="html" xml:base="https://vkorotkine.github.io/blog/2021/identities/"><![CDATA[<p>The following is a random collection of identities and facts I encounter semi-regularly in my research.</p> <h2 id="lie-groups">Lie Groups</h2> <p>Lie groups are ubiquitous in robotics due to the need to handle rotations. If you are looking for a comprehensive introduction to the subject, <d-cite key="solà2021microlietheorystate"></d-cite> and <d-cite key="barfoot2024state"></d-cite> are classic resources.</p> <p>Fact 1: For the $SO(2)$ group of 2D rotations, the following holds, \begin{align} v^\wedge \mathbf{u} &amp;= \begin{bmatrix} 0 &amp; -v\\ v &amp; 0 \end{bmatrix} \begin{bmatrix} u_1 \\ u_2 \end{bmatrix} \\ &amp;= \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix} \begin{bmatrix} u_1 \\ u_2 \end{bmatrix} v \\ &amp;= \mathbf{P} \mathbf{u}v, \end{align} Where \(\mathbf{P}=\begin{bmatrix} 0 &amp; -1 \\\ 0 &amp; 1\end{bmatrix}\). This contrasts with the $SO(3)$ case, where \(\mathbf{v}^\wedge \mathbf{u} = -\mathbf{u}^\wedge \mathbf{v}\). This ties into the $\ \odot\ $ operator as described in <d-cite key="barfoot2024state"></d-cite> in the section covering homogeneous points.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[quick mathematical facts I rederive every so often]]></summary></entry></feed>